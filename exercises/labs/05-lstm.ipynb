{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio 5: LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab5: Recurrent Neural Network and LSTM\n",
        "\n",
        "## Introduction\n",
        "Sometimes, datapoints have time-dependent relationship.\n",
        "\n",
        "* Audio sequence\n",
        "* Text\n",
        "* Video\n",
        "\n",
        "In those situations, a datapoint have to be represented as $\\{ x_t \\}_{t=1}^T$, a sequence of datapoints over time. When we want to train a neural network on those kind of data, we need to exploit that temporal relationship (a word in a sentence, out of context, does mean nothing).\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are specific kind on Networks, mainly created for those kind of applications.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "In a classical RNN model, each recurrent cell has an output $h_t$ for each time step $t>0$, and an internal memory, usually represented as $C_t$. To preserve temporal informations and simplify backpropagation, usually $C_t = h_{t-1}$ for any $t>1$. \\\\\n",
        "\n",
        "Even if this approach works perfectly in theory, in practice there are some severe issues when the time interval is large (i.e. when $T$ is big), such as **vanishing gradient** or **exploding gradient**. This is a consequence that, in backpropagation with a $T$ time-step RNN, the gradient of a loss function $\\ell(\\{ x_t \\}, \\{ y_t \\})$ w.r.t. a weight $w$ is:\n",
        "\n",
        "$$\n",
        "\\Bigl(\\frac{∂ℓ}{∂w}\\Bigr)^T\n",
        "$$\n",
        "\n",
        "Thus, if $\\frac{∂ℓ}{∂w}$ is low, this gradient vanishes for high $T$, while if it is large, it explodes for high $T$. \\\\\n",
        "\n",
        "LSTM solves this issue by limiting the effective time-interval of the model, by letting the cell _lose memory_ during iterations, removing informations to $C_t$ over time. \\\\\n",
        "\n",
        "![](https://miro.medium.com/max/662/1*mcHP77YF63SuqUGAIiBBsA.jpeg)\n",
        "\n",
        "Thus, in LSTM we have two **different** output informations for each $t$: the _state_ $h_t$ and the memory cell $C_t$. Differently from classical RNNs, those are usually not the same. "
      ],
      "metadata": {
        "id": "uBkXn333_Ftm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case of study: text-to-text translation\n",
        "Application of RNNs (and, in particular, LSTM) are multiple:\n",
        "\n",
        "* Time-series Classification\n",
        "* NLP\n",
        "* Text-to-Text translation\n",
        "* ...\n",
        "\n",
        "We will consider a text-to-text translation example, trying to use an LSTM to translate a sentence from English to French."
      ],
      "metadata": {
        "id": "WF2fZxffDcgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yY9RRnth-iEs"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as ks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Prepare the data\n",
        "\n",
        "We will use a dataset from the Tab-Delimited Bilingual Sentence Pairs (TBSP) dataset (http://www.manythings.org/anki/). There, you can find a huge amount of dataset of short sentences from English to other languages, we can use for this task. Of course (since we are patriotic), we will choose English to Italian."
      ],
      "metadata": {
        "id": "F7HntyKoFC9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip the data\n",
        "!curl -O http://www.manythings.org/anki/ita-eng.zip\n",
        "!unzip ita-eng.zip"
      ],
      "metadata": {
        "id": "NMCSbS4oFrSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset file is a $\\texttt{.txt}$ file, which is described on the official documentation to be in the following form:\n",
        "\n",
        "**English + TAB + The Other Language + TAB + Attribution**\n",
        "\n",
        "where the **Attribution** part can be ignored. Thus, this dataset uses **TAB** the split the input from the output, and **\\n** to split different datapoints. \\\\\n",
        "\n",
        "We need to do:\n",
        "* Read the data and create lists containing input and output sequences splitten apart.\n",
        "* Encode the sentences in a matrix form, using for example a one-hot encode algorithm."
      ],
      "metadata": {
        "id": "rjoUz6TvF2a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_file(path, n_samples=10_000):\n",
        "\n",
        "    # Vectorize the data.\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.read().split(\"\\n\")\n",
        "\n",
        "    for line in lines[: min(n_samples, len(lines) - 1)]:\n",
        "        input_text, target_text, _ = line.split(\"\\t\")\n",
        "\n",
        "        # We use \"tab\" as the \"start sequence\" character\n",
        "        # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "        target_text = \"\\t\" + target_text + \"\\n\"\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "\n",
        "    return input_texts, target_texts\n",
        "\n",
        "\n",
        "def compute_unique_characters(texts):\n",
        "    characters = set()\n",
        "\n",
        "    for text in texts:\n",
        "        for char in text:\n",
        "            if char not in characters:\n",
        "                characters.add(char)\n",
        "    \n",
        "    characters = sorted(list(characters))\n",
        "    return characters\n",
        "\n",
        "def one_hot_encode():\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "\n",
        "# Read data\n",
        "input_texts, target_texts = read_data_from_file('ita.txt')\n",
        "input_characters = compute_unique_characters(input_texts)\n",
        "target_characters = compute_unique_characters(target_texts)\n",
        "\n",
        "# Compute useful measures\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "# Encode data\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = one_hot_encode()\n",
        "\n",
        "print('\\n')\n",
        "print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
        "print(\"Shape of decoder input data:\", decoder_input_data.shape)\n",
        "print(\"Shape of decoder target data:\", decoder_target_data.shape)"
      ],
      "metadata": {
        "id": "ZzM1HbO0F12I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore the data\n",
        "\n",
        "Just take a look at the data to understand how it works."
      ],
      "metadata": {
        "id": "zsaRW11C2ISz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Starting data\n",
        "print(f\"{input_texts[1]} -> {target_texts[1]}\")\n",
        "print(f\"{input_texts[5]} -> {target_texts[5]}\")\n",
        "print(f\"{input_texts[30]} -> {target_texts[30]}\")\n",
        "\n",
        "# Encoding\n",
        "for char in encoder_input_data[5]:\n",
        "    idx = np.where(char==1.0)[0][0]\n",
        "    print(f\"{char} -> {input_characters[idx]}\")\n",
        "\n",
        "# NOTE: Empty character = [1, 0, 0, ..., 0]"
      ],
      "metadata": {
        "id": "rvsQ_qV32Mmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "Our model (referred to as Sequence-to-Sequence Learning (Seq2Seq)), works in the following way:\n",
        "\n",
        "\n",
        "1.   Consider two languages Lan1 and Lan2, and suppose we want to translate from Lan1 -> Lan2. \n",
        "2.   Create two LSTM models, referred to as Encoder and Decoder.\n",
        "3.   Train the Encoder to map strings from Lan1 to an encoded version of them, ELan1. This is required to capture local informations (translation character by character is not possible).\n",
        "4.   Ignore the output of the Encoder and keep the states $(h_e, C_e)$, which will be used as an Input for the states $(h_0, C_0)$ of the Decoder.\n",
        "5.   The Decoder takes as input the states from the Encoder (i.e. the encoded string in ELan1) and the string from Lan2, to produce a character of the output translated sequence. Append that character to the output sequence.\n",
        "6.   Repeat the process until a STOP character or the maximum length of the output string is reached. \n",
        "\n",
        "This process is called _teacher forcing_, since the input sequence of the decoder is not changed during training (the target translated sequence is repeatedly given as input).\n",
        "\n",
        "![](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "b3r3aOWH0CoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the model\n",
        "latent_dim = 256\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = ks.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = ks.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = ks.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = ks.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "# Choose a character using softmax as output\n",
        "decoder_dense = ks.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = ks.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "6NWD0mJHz-zI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the model\n",
        "ks.utils.plot_model(model)"
      ],
      "metadata": {
        "id": "RphzAJQO6SP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "# Train\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "# Save weights\n",
        "model.save_weights('seq2seq.h5')"
      ],
      "metadata": {
        "id": "DuWN14Ue6av0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize overfitting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.title('Plot of Loss over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['acc', 'val_acc'])\n",
        "plt.title('Plot of Accuracy over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xqdlnhXy9Ki0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We now want to evaluate our model.\n"
      ],
      "metadata": {
        "id": "MKejR1ll7OFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to split our model in the two part: encoder and decoder.\n",
        "\n",
        "# Build the Encoder model\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = ks.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Build the decoder part\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = ks.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = ks.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = ks.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "\n",
        "# Decode sequences back to something readable.\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "H_1_seME7FqJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ],
      "metadata": {
        "id": "ZNdN3MnL95Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode a new string\n",
        "def encode_string(s):\n",
        "    encoded_s = np.zeros((1, max_encoder_seq_length, num_encoder_tokens))\n",
        "\n",
        "    for t, char in enumerate(s):\n",
        "        encoded_s[0, t, input_token_index[char]] = 1.0\n",
        "    encoded_s[0, t+1:, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    return encoded_s\n",
        "\n",
        "# Try it out (remember the max length!!)\n",
        "s = \"Hello.\"\n",
        "e_s = encode_string(s)\n",
        "\n",
        "# Translate\n",
        "print(f\"{s} -> {decode_sequence(e_s)}\")"
      ],
      "metadata": {
        "id": "GJ4z9NUY_bw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}