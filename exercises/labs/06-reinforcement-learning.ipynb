{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6: Reinforcement Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab6: Reinforcement Learning\n",
        "\n",
        "In Reinforcement Learning (RL), the goal is to teach an Agent to achieve pre-determined tasks, interacting with an Environment. \n",
        "\n",
        "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
        "\n",
        "the key-concepts here are the following:\n",
        "\n",
        "* Given a space of possible States $S$ (the configurations of the Agent in the Environment) and a set of possible actions $A$, assume the Agent is in the state $s_t \\in S$ at time $t > 0$.\n",
        "* According to a given policy $\\pi(a_t|s_t)$, the Agent choose an action $a_t$, and send the state-action pair $(s_t, a_t)$ to the Environment, which gives back a pair $(r_{t+1}, s_{t+1})$ where $r_{t+1}$ is the reward got by executing the action $a_t$ while being in the state $s_t$, while $s_{t+1}$ is the new state where the Agent lives. The process of sampling the new state $s_{t+1}$ given the actual state $s_t$ and an action $a_t$ can be modelled as a probability distribution $P(s_{t+1}|(s_t, a_t))$.\n",
        "* Repeat. \n",
        "\n",
        "Model-based methods tries to learn some proprieties of the Environment $P(s_{t+1}|(s_t, a_t))$ to have a knowledge of what they should do. Model-free methods combine exploring and exploiting to learn what to do to achieve the tasks.\n",
        "\n",
        "## Q-function and Value function\n",
        "An important concept in RL is the concept of Q-function. Given an $(s, a)$ pair and a policy $\\pi$, the Q-function $Q^\\pi(s, a)$ is the discounted expected reward of executing action $a$ while being on a state $s$.\n",
        "\n",
        "$$\n",
        "Q^\\pi(s, a) = \\mathbb{E} \\sum_{t\\geq 0} \\gamma^t r_t\n",
        "$$\n",
        "\n",
        "where $\\gamma > 0$ is the discount factor. When the State-Action space is finite, a Q-function $Q^\\pi(s, a)$ is implemented as a matrix, with $|S|$ rows and $|A|$ columns. \\\\\n",
        "\n",
        "Given a Q-function $Q^\\pi(s, a)$, we define the Value function\n",
        "\n",
        "$$\n",
        "V^\\pi(s) = \\sum_a \\pi(a|s) Q(s, a) = \\mathbb{E}_{a ∼ \\pi(s|a)} Q(s, a)\n",
        "$$\n",
        "\n",
        "## Bellman equation\n",
        "Given a Q-function $Q^\\pi(s, a)$ for any policy $\\pi$, we define the Optimal Q-value function as\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
        "$$\n",
        "\n",
        "and, consequently, the optimal policy $\\pi^*$ as the policy realizing $Q^*(s, a)$. \\\\\n",
        "\n",
        "The optimal Q-value function follows the Bellman equation\n",
        "\n",
        "$$\n",
        "Q^*(s, a) = \\mathbb{E}_{s'}\\Bigl[ r_0 + \\gamma \\max_{a'} Q^*(s', a') \\Bigr]\n",
        "$$\n",
        "\n",
        "## Deep Reinforcement Learning (DRL)\n",
        "A standard choice for the policy (for each algorithm that do not explicitly learns the policy) is the $ϵ$-greedy policy, defined as:\n",
        "\n",
        "$$\n",
        "\\pi(a|s) = \\begin{cases}\n",
        "\\text{random} \\qquad &\\text{ with probability } ϵ \\\\\n",
        "\\arg\\max_a Q(s, a) \\qquad &\\text{ with probability } 1-ϵ\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Given that, when the State-Action space is very high-dimensional (or, when it is continuous), it is basically impossible to learn and store the entire matrix $Q^\\pi(s, a)$ for each $(s, a)$ pair. As a consequence, we can use Neural Networks to learn an approximation of $Q^\\pi(s, a)$, namely $Q_\\theta(s, a)$. Those approaches are called $Q$-learning.\n",
        "\n",
        "## Policy-gradient\n",
        "A common alternative to $Q$-learning consist in learning to approximate the policy $\\pi(a|s)$ as $\\pi_\\theta(a|s)$ instead of approximating $Q^\\pi(s, a)$ and fixing $\\pi(a|s)$ to be the $ϵ$-greedy, as it is common in $Q$-learning. \\\\\n",
        "\n",
        "In each case, training is almost everytime a variant of a fixed-point iteration scheme on Bellman equation, trying to find a set of weights $\\theta$ such that\n",
        "\n",
        "$$\n",
        "Q_\\theta(s, a) ≈ \\mathbb{E}_{s'}\\Bigl[ r_0 + \\gamma \\max_{a'} Q_\\theta(s', a') \\Bigr]\n",
        "$$"
      ],
      "metadata": {
        "id": "WIT9khOsykk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case of study: MicroRacer\n",
        "In our experiments, we will use a benchmark environment developed by professor Andrea Asperti together with Marco Del Brutto, specifically designed for didattic training of Reinforcement Learning models. Indeed, it is known that classical environments such as AWS Deep Racer (from Amazon), Torcs, Learn-to-Race, ... are either too simple to be beaten or too complex, requiring even days of training to be beaten. \\\\\n",
        "\n",
        "MicroRacer it is a perfect environment to understand and tries different algorithms, being not too easy and not too hard. You can find documentation of it at https://github.com/asperti/MicroRacer, together with the related paper https://arxiv.org/abs/2203.10494. \\\\"
      ],
      "metadata": {
        "id": "XdrrFDUzwUEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go into it. Start by cloning the git to download everything we need."
      ],
      "metadata": {
        "id": "YJM3i2dmxZtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBIZH9rEyhLd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/asperti/MicroRacer.git\n",
        "\n",
        "# Remember to modify tracks.py."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MicroRacer is an environment where a random track is randomly generated, with some settings such as: the width of the track, whether or not there are obstacles or chicanes, if there is a limit in the turning of the track, ...\n",
        "\n",
        "![](https://user-images.githubusercontent.com/15980090/135791705-cd678320-c189-43b5-84fe-1ceb0dd01f0d.png)\n",
        "\n",
        "On that track, a number of different cars (models) $m_1, m_2, m_3, m_4$, compete to complete it in the least possible time, without getting out of the street. \\\\\n",
        "\n",
        "Here, each model does not know the shape of the track, but just a small portion of leidar-like view in from of them, defined as an array of 19 views uniformly sampled in the $[-30°, 30°]$ range, plus their scalar velocity $v$. \\\\\n",
        "\n",
        "At each time $t>0$, given their state $s_t$, each car have to reply with two actiosn $a_t = (a_t^1, a_t^2)$, both of them contiuous in range $[-1, 1]$, respectively defining the accelleration at time $t$ and the turning angle (the angular acceleration) at time $t$. \\\\\n",
        "They used angular acceleration instead of absolute turning angle so that it will be harder to turn if the scalar velocity at time $t$ is too high. \\\\\n",
        "\n",
        "The reward is fixed to be the average speed used to get to the end. A negative reward is given if the car ends before getting to the end, either by going out of the track or for being too slow."
      ],
      "metadata": {
        "id": "S20nU9mqxt44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from MicroRacer import tracks \n",
        "\n",
        "# Define the racer object. This is the main class of the library.\n",
        "racer = tracks.Racer(obstacles=True,\n",
        "                     turn_limit=True,\n",
        "                     chicanes=True,\n",
        "                     low_speed_termination=True)\n",
        "\n",
        "# Generate a new track and get the starting state s_0\n",
        "start_state = racer.reset()\n",
        "\n",
        "# Parameters\n",
        "num_states = 5 #we reduce the state dim through observation\n",
        "num_actions = 2 #acceleration and steering\n",
        "print(\"State Space dim: {}, Action Space dim: {}\".format(num_states,num_actions))\n",
        "\n",
        "upper_bound = 1\n",
        "lower_bound = -1\n",
        "print(\"Min and Max Value of Action: {}\".format(lower_bound,upper_bound))"
      ],
      "metadata": {
        "id": "PDn_b8Ag1c6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the problem"
      ],
      "metadata": {
        "id": "uqWM9jkP35Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Discount factor\n",
        "gamma = 0.99\n",
        "\n",
        "# Buffer settings\n",
        "buffer_dim = 50000\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "EumHI3oS34R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model1: DDPG\n",
        "DDPG (Deep Deterministic Policy Gradient) is the continuous extension of Deep Q-Learning to the continuous action-state setup. It is here implemented by an Actor-Critic setup, where the Actor takes as input the state $s_t$ and decides the action $a_t$, while the critic takes the state $s_t$ and the action, decided by the Actor, and returns the Q-value $Q(s_t, a_t)$.\n",
        "\n",
        "![](https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/assets/653820f1-5d3d-4a8f-bc05-a3ac07895e44.png)\n",
        "\n",
        "More informations at http://proceedings.mlr.press/v32/silver14.pdf"
      ],
      "metadata": {
        "id": "ommYk_YO4Bi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The actor choose the move, given the state\n",
        "def get_actor(train_acceleration=True,train_direction=True):\n",
        "    # the actor has separate towers for action and speed\n",
        "    # in this way we can train them separately\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out1 = layers.Dense(32, activation=\"relu\", trainable=train_acceleration)(inputs)\n",
        "    out1 = layers.Dense(32, activation=\"relu\", trainable=train_acceleration)(out1)\n",
        "    out1 = layers.Dense(1, activation='tanh', trainable=train_acceleration)(out1)\n",
        "\n",
        "    out2 = layers.Dense(32, activation=\"relu\", trainable=train_direction)(inputs)\n",
        "    out2 = layers.Dense(32, activation=\"relu\",trainable=train_direction)(out2)\n",
        "    out2 = layers.Dense(1, activation='tanh',trainable=train_direction)(out2)\n",
        "\n",
        "    outputs = layers.concatenate([out1,out2])\n",
        "\n",
        "    #outputs = outputs * upper_bound #resize the range, if required\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"actor\")\n",
        "    return model\n",
        "\n",
        "# The critic compute the q-value, given the state and the action\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(64, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(64, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out) #Outputs single value\n",
        "\n",
        "    model = tf.keras.Model([state_input, action_input], outputs, name=\"critic\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#We compose actor and critic in a single model.\n",
        "#The actor is trained by maximizing the future expected reward, estimated\n",
        "#by the critic. The critic should be freezed while training the actor.\n",
        "#For simplicitly, we just use the target critic, that is not trainable.\n",
        "def compose(actor,critic):\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    a = actor(state_input)\n",
        "    q = critic([state_input,a])\n",
        "\n",
        "    m = tf.keras.Model(state_input, q)\n",
        "    #the loss function of the compound model is just the opposite of the critic output\n",
        "    m.add_loss(-q)\n",
        "    return(m)\n",
        "\n",
        "    \n",
        "\n",
        "# creating models\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "# we create the target model for double learning (to prevent a moving target phenomenon)\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# make target models non trainable.\n",
        "target_actor.trainable = False\n",
        "target_critic.trainable = False\n",
        "\n",
        "# Compose the models in a single one.\n",
        "aux_model = compose(actor_model,target_critic)"
      ],
      "metadata": {
        "id": "Ekyi2M845PnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buffer\n",
        "\n",
        "The buffer is used to reduce the complexity of the algorithm. It saves values for the experience (already tried scenarios) by memorizing tuples $(s_t, a_t, r_t, D, s_{t+1})$ where $D$ is a boolean variable representing if the model achieved the goal or not. "
      ],
      "metadata": {
        "id": "jIrQCTm26kdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Replay buffer\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Max Number of tuples that can be stored\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples used for training\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Current number of tuples in buffer\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # We have a different array for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Stores a transition (s,a,r,s') in the buffer\n",
        "    def record(self, obs_tuple):\n",
        "        s,a,r,T,sn = obs_tuple\n",
        "        # restart form zero if buffer_capacity is exceeded, replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = tf.squeeze(s)\n",
        "        self.action_buffer[index] = a\n",
        "        self.reward_buffer[index] = r\n",
        "        self.done_buffer[index] = T\n",
        "        self.next_state_buffer[index] = tf.squeeze(sn)\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def sample_batch(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        s = self.state_buffer[batch_indices]\n",
        "        a = self.action_buffer[batch_indices]\n",
        "        r = self.reward_buffer[batch_indices]\n",
        "        T = self.done_buffer[batch_indices]\n",
        "        sn = self.next_state_buffer[batch_indices]\n",
        "        return ((s,a,r,T,sn))"
      ],
      "metadata": {
        "id": "ckHr3Nt76gBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the policy"
      ],
      "metadata": {
        "id": "EtEPQRm47kjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(state, verbose=False):\n",
        "    #the policy used for training just add noise to the action\n",
        "    #the amount of noise is kept constant during training\n",
        "    sampled_action = tf.squeeze(actor_model(state))\n",
        "    noise = np.random.normal(scale=0.1,size=2)\n",
        "\n",
        "    #we may change the amount of noise for actions during training\n",
        "    noise[0] *= 2\n",
        "    noise[1] *= .5\n",
        "\n",
        "    # Adding noise to action\n",
        "    sampled_action = sampled_action.numpy()\n",
        "    sampled_action += noise\n",
        "\n",
        "    #in verbose mode, we may print information about selected actions\n",
        "    if verbose and sampled_action[0] < 0:\n",
        "        print(\"decelerating\")\n",
        "\n",
        "    #Finally, we ensure actions are within bounds\n",
        "    legal_action = np.clip(sampled_action, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "metadata": {
        "id": "gvTInWQR7jQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the first Model"
      ],
      "metadata": {
        "id": "7g5cvbPq7uCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "total_iterations = 50_000\n",
        "\n",
        "# Target network parameter update factor, for double DQN\n",
        "tau = 0.005\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.001\n",
        "aux_lr = 0.001\n",
        "\n",
        "is_training = False\n",
        "\n",
        "load_weights = True\n",
        "save_weights = False\n",
        "\n",
        "weights_file_actor = \"./MicroRacer/weights/ddpg_actor_model_car\"\n",
        "weights_file_critic = \"./MicroRacer/weights/ddpg_critic_model_car\"\n",
        "\n",
        "\n",
        "\n",
        "# Slowly updating target parameters according to the tau rate <<1\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def update_weights(target_weights, weights, tau):\n",
        "    return(target_weights * (1- tau) +  weights * tau)\n",
        "\n",
        "\n",
        "\n",
        "## TRAINING ##\n",
        "if load_weights:\n",
        "    critic_model = keras.models.load_model(weights_file_critic)\n",
        "    actor_model = keras.models.load_model(weights_file_actor)\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor_weights = actor_model.get_weights()\n",
        "target_critic_weights = critic_model.get_weights()\n",
        "target_actor.set_weights(target_actor_weights)\n",
        "target_critic.set_weights(target_critic_weights)\n",
        "\n",
        "# Define the optimizer\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "aux_optimizer = tf.keras.optimizers.Adam(aux_lr)\n",
        "\n",
        "# Compile the models\n",
        "critic_model.compile(loss='mse',optimizer=critic_optimizer)\n",
        "aux_model.compile(optimizer=aux_optimizer)\n",
        "\n",
        "# Create the buffer\n",
        "buffer = Buffer(buffer_dim, batch_size)\n",
        "\n",
        "# History of rewards per episode\n",
        "ep_reward_list = []\n",
        "# Average reward history of last few episodes\n",
        "avg_reward_list = []         \n",
        "\n",
        "# We introduce a probability of doing n empty actions to separate the environment time-step from the agent\n",
        "def step(action):\n",
        "    n = 1\n",
        "    t = np.random.randint(0,n)\n",
        "    state ,reward,done = racer.step(action)\n",
        "    for i in range(t):\n",
        "        if not done:\n",
        "            state ,t_r, done = racer.step([0, 0])\n",
        "            #state ,t_r, done =racer.step(action)\n",
        "            reward+=t_r\n",
        "    return (state, reward, done)\n",
        "\n",
        "\n",
        "def train(total_iterations=total_iterations):\n",
        "    i = 0\n",
        "    mean_speed = 0\n",
        "    ep = 0\n",
        "    avg_reward = 0\n",
        "    while i<total_iterations:\n",
        "\n",
        "        prev_state = racer.reset()\n",
        "        episodic_reward = 0\n",
        "        mean_speed += prev_state[num_states-1]\n",
        "        done = False\n",
        "        while not(done):\n",
        "            i = i+1\n",
        "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "            #our policy is always noisy\n",
        "            action = policy(tf_prev_state)[0]\n",
        "            # Get state and reward from the environment\n",
        "            state, reward, done = step(action)\n",
        "            \n",
        "            #we distinguish between termination with failure (state = None) and succesfull termination on track completion\n",
        "            #succesfull termination is stored as a normal tuple\n",
        "            fail = done and len(state)<num_states \n",
        "            buffer.record((prev_state, action, reward, fail, state))\n",
        "            if not(done):\n",
        "                mean_speed += state[num_states-1]\n",
        "        \n",
        "            episodic_reward += reward\n",
        "\n",
        "            if buffer.buffer_counter>batch_size:\n",
        "                states,actions,rewards,dones,newstates= buffer.sample_batch()\n",
        "                targetQ = rewards + (1-dones)*gamma*(target_critic([newstates,target_actor(newstates)]))\n",
        "                loss1 = critic_model.train_on_batch([states,actions],targetQ)\n",
        "                loss2 = aux_model.train_on_batch(states)\n",
        "\n",
        "                update_target(target_actor.variables, actor_model.variables, tau)\n",
        "                update_target(target_critic.variables, critic_model.variables, tau)\n",
        "            prev_state = state\n",
        "            \n",
        "            if i%100 == 0:\n",
        "                avg_reward_list.append(avg_reward)\n",
        "                \n",
        "\n",
        "        ep_reward_list.append(episodic_reward)\n",
        "\n",
        "        # Mean of last 40 episodes\n",
        "        avg_reward = np.mean(ep_reward_list[-40:])\n",
        "        print(\"Episode {}: Iterations {}, Avg. Reward = {}, Last reward = {}. Avg. speed = {}\".format(ep, i, avg_reward,episodic_reward,mean_speed/i))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if ep>0 and ep%40 == 0:\n",
        "            print(\"## Evaluating policy ##\")\n",
        "            tracks.metrics_run(actor_model, 10)\n",
        "        ep += 1\n",
        "        \n",
        "\n",
        "    if total_iterations > 0:\n",
        "        if save_weights:\n",
        "            critic_model.save(weights_file_critic)\n",
        "            actor_model.save(weights_file_actor)\n",
        "        # Plotting Episodes versus Avg. Rewards\n",
        "        plt.plot(avg_reward_list)\n",
        "        plt.xlabel(\"Training steps x100\")\n",
        "        plt.ylabel(\"Avg. Episodic Reward\")\n",
        "        plt.ylim(-3.5,7)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.001)\n",
        "        print(\"### DDPG Training ended ###\")\n",
        "        print(\"Trained over {} steps\".format(i))\n",
        "\n",
        "# Rung training\n",
        "if is_training:\n",
        "    start_t = datetime.now()\n",
        "    train()\n",
        "    end_t = datetime.now()\n",
        "    print(\"Time elapsed: {}\".format(end_t-start_t))"
      ],
      "metadata": {
        "id": "8mNnWp2W0Ark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model we will use to compete is the Actor model\n",
        "ddpg_model = actor_model"
      ],
      "metadata": {
        "id": "0NkPynXwDH1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model2: DDPG2\n",
        "\n",
        "DDPG2 is a variant of DDPG where noise is added to the parameter to stabilize training."
      ],
      "metadata": {
        "id": "ImVTGbO_EUk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model"
      ],
      "metadata": {
        "id": "u7pkbkh9EmXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "total_iterations = 50000\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.99\n",
        "\n",
        "# Target network parameter update factor, for double DQN\n",
        "tau = 0.005\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.001\n",
        "aux_lr = 0.001\n",
        "\n",
        "param_noise_stddev = 0.2\n",
        "\n",
        "is_training = False\n",
        "\n",
        "load_weights = True\n",
        "save_weights = False #beware when saving weights to not overwrite previous data\n",
        "\n",
        "weights_file_actor = \"MicroRacer/weights/ddpg2_actor_model_car\"\n",
        "weights_file_critic = \"MicroRacer/weights/ddpg2_critic_model_car\"\n",
        "\n",
        "\n",
        "#The actor choose the move, given the state\n",
        "def get_actor():\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(64, name=\"perturbable1\", activation=\"relu\")(inputs)\n",
        "    out = layers.LayerNormalization()(out)\n",
        "    out = layers.Dense(64, name=\"perturbable2\", activation=\"relu\")(out)\n",
        "    out = layers.LayerNormalization()(out)\n",
        "    outputs = layers.Dense(num_actions, name=\"perturbable3\", activation=\"tanh\")(out)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"actor\")\n",
        "    return model\n",
        "    \n",
        "\n",
        "#the critic compute the q-value, given the state and the action\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(64, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(64, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out) #Outputs single value\n",
        "\n",
        "    model = tf.keras.Model([state_input, action_input], outputs, name=\"critic\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#creating models\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "#we create the target model for double learning (to prevent a moving target phenomenon)\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "target_actor.trainable = False\n",
        "target_critic.trainable = False\n",
        "\n",
        "#We compose actor and critic in a single model.\n",
        "#The actor is trained by maximizing the future expected reward, estimated\n",
        "#by the critic. The critic should be freezed while training the actor.\n",
        "#For simplicitly, we just use the target critic, that is not trainable.\n",
        "def compose(actor,critic):\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    a = actor(state_input)\n",
        "    q = critic([state_input,a])\n",
        "    #reg_weights = actor.get_layer('out').get_weights()[0]\n",
        "    #print(tf.reduce_sum(0.01 * tf.square(reg_weights)))\n",
        "\n",
        "    m = tf.keras.Model(state_input, q)\n",
        "    #the loss function of the compound model is just the opposite of the critic output\n",
        "    m.add_loss(-q)\n",
        "    return(m)\n",
        "\n",
        "# Updates weigths of the perturbed actor inserting noise too\n",
        "def get_perturbed_actor_updates(actor, perturbed_actor, param_noise_stddev):\n",
        "    updates = []\n",
        "    for var, perturbed_var in zip(actor.trainable_variables, perturbed_actor.trainable_variables):\n",
        "        if \"perturbable\" in var.name:\n",
        "            updates.append(perturbed_var.assign( var + tf.random.normal(tf.shape(var), mean=0., stddev=param_noise_stddev)))\n",
        "    return tf.group(*updates)\n",
        "\n",
        "# Calculates distance between perturbed actor and clean actor and uses it to update standard deviation\n",
        "def adapt_param_noise(actor, adaptive_actor, states,current_stddev):\n",
        "    adoption_coefficient=1.01\n",
        "    # Perturb a separate copy of the policy to adjust the scale for the next \"real\" perturbation.\n",
        "    perturb_adaptive_policy_ops = get_perturbed_actor_updates(actor, adaptive_actor, current_stddev)\n",
        "    distance = tf.sqrt(tf.reduce_mean(tf.square(actor(states) - adaptive_actor(states))))\n",
        "    \n",
        "    if distance > current_stddev:\n",
        "        # Decrease stddev.\n",
        "        current_stddev /= adoption_coefficient\n",
        "    else:\n",
        "        # Increase stddev.\n",
        "        current_stddev *= adoption_coefficient\n",
        "    return current_stddev\n",
        "\n",
        "aux_model = compose(actor_model,target_critic)\n",
        "\n",
        "# Configure perturbed actor.\n",
        "param_noise_actor = get_actor()\n",
        "perturb_policy_ops = get_perturbed_actor_updates(actor_model, param_noise_actor, param_noise_stddev)\n",
        "\n",
        "# Configure separate copy for stddev adoption.\n",
        "adaptive_param_noise_actor = get_actor()\n",
        "perturb_adaptive_policy_ops = get_perturbed_actor_updates(actor_model, adaptive_param_noise_actor, param_noise_stddev)"
      ],
      "metadata": {
        "id": "ezpCx4z_ElnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replay buffer\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Max Number of tuples that can be stored\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples used for training\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Current number of tuples in buffer\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # We have a different array for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Stores a transition (s,a,r,s') in the buffer\n",
        "    def record(self, obs_tuple):\n",
        "        s,a,r,T,sn = obs_tuple\n",
        "        # restart form zero if buffer_capacity is exceeded, replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = tf.squeeze(s)\n",
        "        self.action_buffer[index] = a\n",
        "        self.reward_buffer[index] = r\n",
        "        self.done_buffer[index] = T\n",
        "        self.next_state_buffer[index] = tf.squeeze(sn)\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def sample_batch(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        s = self.state_buffer[batch_indices]\n",
        "        a = self.action_buffer[batch_indices]\n",
        "        r = self.reward_buffer[batch_indices]\n",
        "        T = self.done_buffer[batch_indices]\n",
        "        sn = self.next_state_buffer[batch_indices]\n",
        "        return ((s,a,r,T,sn))\n",
        "\n",
        "buffer = Buffer(buffer_dim, batch_size)"
      ],
      "metadata": {
        "id": "MJl_ojK7ErFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "B_L5-lVxFWDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slowly updating target parameters according to the tau rate <<1\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def update_weights(target_weights, weights, tau):\n",
        "    return(target_weights * (1- tau) +  weights * tau)\n",
        "\n",
        "def policy(state,verbose=False):\n",
        "    add_action_noise = False\n",
        "    # we use the actor modified with parametric noise to generate the action\n",
        "    sampled_action = tf.squeeze(param_noise_actor(state))\n",
        "    if add_action_noise:\n",
        "        noise = np.random.normal(scale=0.1,size=2)\n",
        "        #we may change the amount of noise for actions during training\n",
        "        noise[0] *= 2\n",
        "        noise[1] *= .5\n",
        "        # Adding noise to action\n",
        "        sampled_action = sampled_action.numpy()\n",
        "        sampled_action += noise\n",
        "    #in verbose mode, we may print information about selected actions\n",
        "    if verbose and sampled_action[0] < 0:\n",
        "        print(\"decelerating\")\n",
        "\n",
        "    #Finally, we ensure actions are within bounds\n",
        "    legal_action = np.clip(sampled_action, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]\n",
        "\n",
        "\n",
        "## TRAINING ##\n",
        "if load_weights:\n",
        "    critic_model = keras.models.load_model(weights_file_critic)\n",
        "    actor_model = keras.models.load_model(weights_file_actor)\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor_weights = actor_model.get_weights()\n",
        "target_critic_weights = critic_model.get_weights()\n",
        "target_actor.set_weights(target_actor_weights)\n",
        "target_critic.set_weights(target_critic_weights)\n",
        "\n",
        "\n",
        "\n",
        "critic_model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(critic_lr))\n",
        "aux_model.compile(optimizer=tf.keras.optimizers.Adam(aux_lr))\n",
        "\n",
        "\n",
        "# History of rewards per episode\n",
        "ep_reward_list = []\n",
        "# Average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "\n",
        "# We introduce a probability of doing n empty actions to separate the environment time-step from the agent   \n",
        "def step(action):\n",
        "    n = 1\n",
        "    t = np.random.randint(0,n)\n",
        "    state ,reward,done = racer.step(action)\n",
        "    for i in range(t):\n",
        "        if not done:\n",
        "            state ,t_r, done =racer.step([0, 0])\n",
        "            #state ,t_r, done =racer.step(action)\n",
        "            reward+=t_r\n",
        "    return (state, reward, done)\n",
        "\n",
        "\n",
        "def train(total_iterations=total_iterations):\n",
        "    i = 0\n",
        "    mean_speed = 0\n",
        "    current_stddev = param_noise_stddev\n",
        "    ep = 0\n",
        "    avg_reward = 0\n",
        "    while i<total_iterations:\n",
        "        prev_state = racer.reset()\n",
        "        episodic_reward = 0\n",
        "        mean_speed += prev_state[4]\n",
        "        done = False\n",
        "        while not(done):\n",
        "            i = i+1\n",
        "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "            #our policy is always noisy\n",
        "            action = policy(tf_prev_state)[0]\n",
        "            # Get state and reward from the environment\n",
        "            state, reward, done = step(action)\n",
        "            \n",
        "            #we distinguish between termination with failure (state = None) and succesfull termination on track completion\n",
        "            #succesfull termination is stored as a normal tuple\n",
        "            fail = done and len(state)<5 \n",
        "            buffer.record((prev_state, action, reward, fail, state))\n",
        "            if not(done):\n",
        "                mean_speed += state[4]\n",
        "        \n",
        "            episodic_reward += reward\n",
        "\n",
        "            if buffer.buffer_counter>batch_size:\n",
        "                states,actions,rewards,dones,newstates= buffer.sample_batch()\n",
        "                targetQ = rewards + (1-dones)*gamma*(target_critic([newstates,target_actor(newstates)]))\n",
        "                loss1 = critic_model.train_on_batch([states,actions],targetQ)\n",
        "                loss2 = aux_model.train_on_batch(states)\n",
        "\n",
        "                update_target(target_actor.variables, actor_model.variables, tau)\n",
        "                update_target(target_critic.variables, critic_model.variables, tau)\n",
        "                \n",
        "                # We calculate adapted standard devation at every step\n",
        "                current_stddev = adapt_param_noise(actor_model, adaptive_param_noise_actor, states,current_stddev)\n",
        "            prev_state = state\n",
        "            \n",
        "            if i%100 == 0:\n",
        "                avg_reward_list.append(avg_reward)\n",
        "        \n",
        "        # We update the perturbed actor parametric noise only after an episode\n",
        "        perturb_policy_ops = get_perturbed_actor_updates(actor_model, param_noise_actor, current_stddev)\n",
        "\n",
        "        ep_reward_list.append(episodic_reward)\n",
        "\n",
        "        # Mean of last 40 episodes\n",
        "        avg_reward = np.mean(ep_reward_list[-40:])\n",
        "        print(\"Episode {}: Iterations {}, Avg. Reward = {}, Last reward = {}. Avg. speed = {}\".format(ep, i, avg_reward,episodic_reward,mean_speed/i))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if ep>0 and ep%40 == 0:\n",
        "            print(\"## Evaluating policy ##\")\n",
        "            tracks.metrics_run(actor_model, 10)\n",
        "        ep += 1\n",
        "\n",
        "    if total_iterations > 0:\n",
        "        if save_weights:\n",
        "            critic_model.save(weights_file_critic)\n",
        "            actor_model.save(weights_file_actor)\n",
        "        # Plotting Episodes versus Avg. Rewards\n",
        "        plt.plot(avg_reward_list)\n",
        "        plt.xlabel(\"Training steps x100\")\n",
        "        plt.ylabel(\"Avg. Episodic Reward\")\n",
        "        plt.ylim(-3.5,7)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.001)\n",
        "        print(\"### DDPG2 Training ended ###\")\n",
        "        print(\"Trained over {} steps\".format(i))\n",
        "\n",
        "if is_training:\n",
        "    start_t = datetime.now()\n",
        "    train()\n",
        "    end_t = datetime.now()\n",
        "    print(\"Time elapsed: {}\".format(end_t-start_t))"
      ],
      "metadata": {
        "id": "FWU0Bd4nEUOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To compete, we will be use the actor model\n",
        "ddpg2_model = actor_model"
      ],
      "metadata": {
        "id": "179loh8JFb7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model3: PPO\n",
        "\n",
        "PPO (Proximal Policy Optimization) is a policy-gradient techniques. Its main difference wrt classical policy-gradient methods is that it clips the objective function to ensure that the deviation from the previous policy is relatively small. \\\\\n",
        "\n",
        "More information at: https://arxiv.org/abs/1707.06347\n"
      ],
      "metadata": {
        "id": "FWoGcMc5-Dy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "BE5Rcccg-Ui2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The actor choose the move, given the state\n",
        "class Get_actor(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = layers.Dense(64, activation=\"tanh\")\n",
        "        self.d2 = layers.Dense(64, activation=\"tanh\")\n",
        "        self.m = layers.Dense(num_actions, activation=\"tanh\")\n",
        "        \n",
        "    def call(self, s):\n",
        "        out = self.d1(s)\n",
        "        out = self.d2(out)\n",
        "        mu = self.m(out)\n",
        "        sigma = 0.2\n",
        "        return  mu, sigma\n",
        "    \n",
        "    @property  \n",
        "    def trainable_variables(self):\n",
        "        return self.d1.trainable_variables + \\\n",
        "                self.d2.trainable_variables + \\\n",
        "                self.m.trainable_variables\n",
        "\n",
        "\n",
        "#the critic compute the value, given the state \n",
        "class Get_critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = layers.Dense(64, activation=\"tanh\")\n",
        "        self.d2 = layers.Dense(64, activation=\"tanh\")\n",
        "        self.o = layers.Dense(1)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        out = self.d1(inputs)\n",
        "        out = self.d2(out)\n",
        "        q = self.o(out)\n",
        "        return q\n",
        "    \n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return self.d1.trainable_variables + \\\n",
        "                self.d2.trainable_variables + \\\n",
        "                self.o.trainable_variables\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.racer = tracks.Racer()\n",
        "        self.actor_model = Get_actor()\n",
        "        self.critic_model = Get_critic()\n",
        "        self.buffer = Buffer(batch_size)     \n",
        "        ## TRAINING ##\n",
        "        self.critic_model(layers.Input(shape=(num_states)))\n",
        "        self.actor_model(layers.Input(shape=(num_states)))\n",
        "        if load_weights:\n",
        "            self.critic_model = keras.models.load_model(weights_file_critic)\n",
        "            self.actor_model = keras.models.load_model(weights_file_actor)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "        self.actor_model.compile(optimizer=self.actor_optimizer)\n",
        "        self.critic_model.compile(loss=\"mse\",optimizer=self.critic_optimizer)\n",
        "        # History of rewards per episode\n",
        "        self.ep_reward_list = []\n",
        "        # Average reward history of last few episodes\n",
        "        self.avg_reward_list = []\n",
        "        # Keep track of how many training steps has been done\n",
        "        \n",
        "    #Returns an action sampled from the normal distribution returned by the actor and it's relative log probability.\n",
        "    #If an action is passed returns it's log probability.\n",
        "    def get_action_and_logp(self, states, actions=None):\n",
        "        mu, sigma = self.actor_model(states)\n",
        "        dist = tfp.distributions.Normal(mu, sigma)\n",
        "        if actions is None:\n",
        "            # Use of the reparameterization trick \n",
        "            actions = mu + sigma * tfp.distributions.Normal(0,1).sample(num_actions)   \n",
        "        log_p = dist.log_prob(actions)\n",
        "        \n",
        "        if len(log_p.shape)>1:\n",
        "            log_p = tf.reduce_sum(log_p,1)\n",
        "        else:\n",
        "            log_p = tf.reduce_sum(log_p)\n",
        "        log_p = tf.expand_dims(log_p, 1)\n",
        "        \n",
        "        valid_action  = K.clip(actions, lower_bound, upper_bound)\n",
        "        \n",
        "        return  valid_action, log_p\n",
        "        \n",
        "        \n",
        "    def gae(self, values, rewards, masks, lastvalue):\n",
        "        returns = []\n",
        "        gae = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            if i==len(rewards)-1:\n",
        "                nextvalue=lastvalue\n",
        "            else:\n",
        "                nextvalue=values[i+1]\n",
        "            delta=rewards[i]+gamma*nextvalue*masks[i]-values[i]  \n",
        "            gae=delta+gamma*gae_lambda*masks[i]*gae\n",
        "            returns.insert(0, gae+values[i])\n",
        "        advantages = returns - values\n",
        "        advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
        "        return np.array(returns), advantages\n",
        "    \n",
        "    def update_networks(self, last_value=0): \n",
        "        states, actions, rewards, dones, values, old_logp, batches = self.buffer.sample_batch()\n",
        "        returns, advantages = self.gae(values, rewards, dones, last_value)     \n",
        "        # Train using mini-batches\n",
        "        for batch in batches:\n",
        "            s_batch = tf.convert_to_tensor(states[batch], dtype=tf.float32)\n",
        "            a_batch = tf.convert_to_tensor(actions[batch], dtype=tf.float32)\n",
        "            adv_batch = tf.expand_dims(tf.convert_to_tensor(advantages.numpy()[batch], dtype=tf.float32),1)\n",
        "            ret_batch =  tf.expand_dims(tf.convert_to_tensor(returns[batch], dtype=tf.float32),1)\n",
        "            ologp_batch = tf.expand_dims(tf.convert_to_tensor(old_logp[batch], dtype=tf.float32),1)\n",
        "            for e in range(epochs):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    tape.watch(self.actor_model.trainable_variables)\n",
        "                    _,logp_batch = self.get_action_and_logp(tf.stack(s_batch), tf.stack(a_batch)) \n",
        "                    ratio = tf.exp(logp_batch-ologp_batch)\n",
        "                    weighted_ratio = ratio*adv_batch\n",
        "                    weighted_clipped_ratio = tf.clip_by_value(ratio, clip_value_min=1- policy_clip, clip_value_max=1+ policy_clip)*adv_batch\n",
        "                    min_wr = tf.minimum(weighted_ratio, weighted_clipped_ratio)- target_entropy*logp_batch\n",
        "                    loss = -tf.reduce_mean(min_wr)            \n",
        "                grad = tape.gradient(loss, self.actor_model.trainable_variables)    \n",
        "                self.actor_model.optimizer.apply_gradients(zip(grad, self.actor_model.trainable_variables))\n",
        "                \n",
        "                c_loss = self.critic_model.train_on_batch(s_batch,ret_batch)\n",
        "                \n",
        "                # We use approximatation of Kullback–Leibler divergence to early stop training epochs\n",
        "                _,logp = self.get_action_and_logp(s_batch, a_batch) \n",
        "                kl = tf.reduce_mean(ologp_batch-logp)\n",
        "                if kl > 1.5*target_kl:\n",
        "                    print(\"early stopping - max kl reached at epoch {}\".format(e))\n",
        "                    break\n",
        "\n",
        "        # We empty the buffer after policy update\n",
        "        self.buffer.clear()\n",
        "     \n",
        "    # We introduce a probability of doing n empty actions to separate the environment time-step from the agent   \n",
        "    def step(self, action):\n",
        "        n = 2\n",
        "        t = np.random.randint(0,n)\n",
        "        state ,reward,done = self.racer.step(action)\n",
        "        for i in range(t):\n",
        "            if not done:\n",
        "                state ,t_r, done = self.racer.step([0, 0])\n",
        "                #state ,t_r, done =racer.step(action)\n",
        "                reward+=t_r\n",
        "        return (state, reward, done)\n",
        "          \n",
        "    def train(self):\n",
        "        i = 0\n",
        "        mean_speed = 0\n",
        "        for ep in range(total_iterations):\n",
        "            state = self.racer.reset()\n",
        "            done = False\n",
        "            episodic_reward = 0\n",
        "            \n",
        "            while not done:    \n",
        "                i+=1\n",
        "                state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
        "                action, logp = self.get_action_and_logp(state)\n",
        "                value = self.critic_model(state)\n",
        "                #action = K.clip(action, lower_bound, upper_bound)\n",
        "                action = tf.squeeze(action)\n",
        "                nstate, reward, done = self.step(action)\n",
        "                self.buffer.record(state, action, reward, not done, value, logp)\n",
        "                if not done:\n",
        "                    mean_speed += nstate[4]\n",
        "                state = nstate\n",
        "                episodic_reward += reward\n",
        "              \n",
        "            # training after a complete episode \n",
        "            self.update_networks()\n",
        "               \n",
        "            self.ep_reward_list.append(episodic_reward) \n",
        "            avg_reward = np.mean(self.ep_reward_list[-40:])\n",
        "            #avg_reward = np.mean(self.ep_reward_list)\n",
        "            print(\"Episode {}: Avg. Reward = {}, Last reward = {}. Avg. speed = {}\".format(ep, avg_reward,episodic_reward,mean_speed/i))\n",
        "            print(\"\\n\")\n",
        "            self.avg_reward_list.append(avg_reward)\n",
        "\n",
        "        if total_iterations > 0:\n",
        "            if save_weights:\n",
        "                self.critic_model.save(weights_file_critic)\n",
        "                self.actor_model.save(weights_file_actor)\n",
        "            # Plotting Episodes versus Avg. Rewards\n",
        "            plt.plot(self.avg_reward_list)\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Avg. Episodic Reward\")\n",
        "            plt.ylim(-3.5,7)\n",
        "            plt.show(block=False)\n",
        "            plt.pause(0.001)\n",
        "        print(\"### PPO Training ended ###\")\n",
        "            \n",
        "\n",
        "    def launch(self):\n",
        "        if is_training:\n",
        "            start_t = datetime.now()\n",
        "            self.train()\n",
        "            end_t = datetime.now()\n",
        "            print(\"Time elapsed: {}\".format(end_t-start_t))"
      ],
      "metadata": {
        "id": "1NqMruNu-XVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trajectories buffer\n",
        "class Buffer:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states=[]\n",
        "        self.actions=[]\n",
        "        self.rewards=[]\n",
        "        self.dones=[]\n",
        "        self.val=[]\n",
        "        self.logp=[]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def sample_batch(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "        return np.array(self.states), np.array(self.actions), np.array(self.rewards),np.array(self.dones), np.array(self.val), np.array(self.logp), batches\n",
        "            \n",
        "    def record(self, state, action, reward, done, val, logp):\n",
        "        self.states.append(tf.squeeze(state))\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "        self.val.append(tf.squeeze(val))\n",
        "        self.logp.append(tf.squeeze(logp))\n",
        "       \n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.rewards.clear()\n",
        "        self.dones.clear() \n",
        "        self.val.clear()\n",
        "        self.logp.clear()"
      ],
      "metadata": {
        "id": "axeiYHNh-nzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_-8JVI2g-vvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate for actor-critic models\n",
        "critic_lr = 3e-4\n",
        "actor_lr = 3e-4\n",
        "\n",
        "# Number of episodes\n",
        "total_iterations = 600\n",
        "\n",
        "# Mini-batch size for training\n",
        "batch_size = 64\n",
        "\n",
        "# Number of training steps with the same episode\n",
        "epochs = 10\n",
        "\n",
        "gamma = 0.99\n",
        "gae_lambda = 0.95\n",
        "policy_clip = tf.constant(0.25, dtype=tf.float32)\n",
        "\n",
        "target_entropy = tf.constant(0.01, dtype=tf.float32)\n",
        "\n",
        "target_kl = 0.01\n",
        "\n",
        "is_training = False\n",
        "\n",
        "load_weights = True\n",
        "save_weights = False\n",
        "\n",
        "weights_file_actor = \"MicroRacer/weights/ppo_actor_model_car\"\n",
        "weights_file_critic = \"MicroRacer/weights/ppo_critic_model_car\"\n",
        "        \n",
        "ppo_agent = Agent()\n",
        "ppo_agent.launch()"
      ],
      "metadata": {
        "id": "Rx3NnXLx-NC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To compete, we will use the actor model\n",
        "ppo_model = ppo_agent.actor_model"
      ],
      "metadata": {
        "id": "vckBOdkLDUCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model4: SAC\n",
        "\n",
        "SAC (Soft Actor-Critic) is a variant of DDPG where the loss function is regularized by adding Entropy. \\\\\n",
        "\n",
        "More informations at: https://arxiv.org/abs/1801.01290"
      ],
      "metadata": {
        "id": "dPDUd9hLBh46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model"
      ],
      "metadata": {
        "id": "5tgXey85CHjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The actor choose the move, given the state\n",
        "class Get_actor(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = layers.Dense(64, activation=\"relu\")\n",
        "        self.d2 = layers.Dense(64, activation=\"relu\")\n",
        "        self.m = layers.Dense(num_actions)\n",
        "        self.s = layers.Dense(num_actions)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        out = self.d1(inputs)\n",
        "        out = self.d2(out)\n",
        "        mu = self.m(out)\n",
        "        log_sigma = self.s(out)\n",
        "        sigma = tf.exp(log_sigma)\n",
        "        \n",
        "        dist = tfp.distributions.Normal(mu, sigma)\n",
        "        #action = dist.sample() \n",
        "        action = mu + sigma * tfp.distributions.Normal(0,1).sample(num_actions)     \n",
        "        valid_action = tf.tanh(action)\n",
        "        \n",
        "        log_p = dist.log_prob(action)\n",
        "        #correct log_p after the tanh squashing on action \n",
        "        log_p = log_p - tf.reduce_sum(tf.math.log(1 - valid_action**2 + 1e-16), axis=1, keepdims=True)\n",
        "        \n",
        "        if len(log_p.shape)>1:\n",
        "            log_p = tf.reduce_sum(log_p,1)\n",
        "        else:\n",
        "            log_p = tf.reduce_sum(log_p)\n",
        "        log_p = tf.reshape(log_p,(-1,1))  \n",
        "        \n",
        "        eval_action = tf.tanh(mu)\n",
        "        \n",
        "        return eval_action, valid_action, log_p\n",
        "    \n",
        "    @property  \n",
        "    def trainable_variables(self):\n",
        "        return self.d1.trainable_variables + \\\n",
        "                self.d2.trainable_variables + \\\n",
        "                self.m.trainable_variables + \\\n",
        "                self.s.trainable_variables\n",
        "\n",
        "\n",
        "#the critic compute the q-value, given the state and the action\n",
        "class Get_critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d1 = layers.Dense(64, activation=\"relu\")\n",
        "        self.d2 = layers.Dense(64, activation=\"relu\")\n",
        "        self.o = layers.Dense(1)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        state, action = inputs\n",
        "        state_action = tf.concat([state, action], axis=1)\n",
        "        out = self.d1(state_action)\n",
        "        out = self.d2(out)\n",
        "        q = self.o(out)\n",
        "        return q\n",
        "    \n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return self.d1.trainable_variables + \\\n",
        "                self.d2.trainable_variables + \\\n",
        "                self.o.trainable_variables\n",
        "\n",
        "\n",
        "#creating models\n",
        "actor_model = Get_actor()\n",
        "critic_model = Get_critic()\n",
        "critic2_model = Get_critic()\n",
        "\n",
        "#we create the target model for double learning (to prevent a moving target phenomenon)\n",
        "target_critic = Get_critic()\n",
        "target_critic2 = Get_critic()\n",
        "target_critic.trainable = False\n",
        "target_critic2.trainable = False"
      ],
      "metadata": {
        "id": "ityGKrgUBwTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replay buffer\n",
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Max Number of tuples that can be stored\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples used for training\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Current number of tuples in buffer\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # We have a different array for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Stores a transition (s,a,r,s') in the buffer\n",
        "    def record(self, obs_tuple):\n",
        "        s,a,r,T,sn = obs_tuple\n",
        "        # restart form zero if buffer_capacity is exceeded, replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = tf.squeeze(s)\n",
        "        self.action_buffer[index] = a\n",
        "        self.reward_buffer[index] = r\n",
        "        self.done_buffer[index] = T\n",
        "        self.next_state_buffer[index] = tf.squeeze(sn)\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def sample_batch(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        s = self.state_buffer[batch_indices]\n",
        "        a = self.action_buffer[batch_indices]\n",
        "        r = self.reward_buffer[batch_indices]\n",
        "        T = self.done_buffer[batch_indices]\n",
        "        sn = self.next_state_buffer[batch_indices]\n",
        "        return ((s,a,r,T,sn))\n",
        "\n",
        "buffer = Buffer(buffer_dim, batch_size)"
      ],
      "metadata": {
        "id": "EF1kSRwVCDgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "_LLCqT_qCLGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Adaptive Entropy to maximize exploration\n",
        "target_entropy = -tf.constant(num_actions, dtype=tf.float32)\n",
        "log_alpha = tf.Variable(0.0, dtype=tf.float32)\n",
        "alpha = tfp.util.DeferredTensor(log_alpha, tf.exp)\n",
        "\n",
        "is_training = False\n",
        "\n",
        "#pesi\n",
        "load_weights = True\n",
        "save_weights = False #beware when saving weights to not overwrite previous data\n",
        "\n",
        "weights_file_actor = \"MicroRacer/weights/sac_actor_model_car\"\n",
        "weights_file_critic = \"MicroRacer/weights/sac_critic_model_car\"\n",
        "weights_file_critic2 = \"MicroRacer/weights/sac_critic2_model_car\"\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Slowly updating target parameters according to the tau rate <<1\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))\n",
        "\n",
        "def update_weights(target_weights, weights, tau):\n",
        "    return(target_weights * (1- tau) +  weights * tau)\n",
        "\n",
        "## TRAINING ##\n",
        "if load_weights:\n",
        "    target_critic([layers.Input(shape=(num_states)),layers.Input(shape=(num_actions))])\n",
        "    target_critic2([layers.Input(shape=(num_states)),layers.Input(shape=(num_actions))])\n",
        "    critic_model = keras.models.load_model(weights_file_critic)\n",
        "    critic2_model = keras.models.load_model(weights_file_critic2)\n",
        "    actor_model = keras.models.load_model(weights_file_actor)\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_critic_weights = critic_model.get_weights()\n",
        "target_critic2_weights = critic2_model.get_weights()\n",
        "target_critic.set_weights(target_critic_weights)\n",
        "target_critic2.set_weights(target_critic2_weights)\n",
        "\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "critic2_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "alpha_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "critic_model.compile(optimizer=critic_optimizer)\n",
        "critic2_model.compile(optimizer=critic2_optimizer)\n",
        "actor_model.compile(optimizer=actor_optimizer)\n",
        "\n",
        "# History of rewards per episode\n",
        "ep_reward_list = []\n",
        "# Average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# We introduce a probability of doing n empty actions to separate the environment time-step from the agent   \n",
        "def step(action):\n",
        "    n = 1\n",
        "    t = np.random.randint(0,n)\n",
        "    action = tf.squeeze(action)\n",
        "    state ,reward,done = racer.step(action)\n",
        "    for i in range(t):\n",
        "        if not done:\n",
        "            state ,t_r, done =racer.step([0, 0])\n",
        "            #state ,t_r, done =racer.step(action)\n",
        "            reward+=t_r\n",
        "    return (state, reward, done)\n",
        "\n",
        "@tf.function \n",
        "def update_critics(states, actions, rewards, dones, newstates):\n",
        "    entropy_scale = tf.convert_to_tensor(alpha)\n",
        "    _, new_policy_actions, log_probs = actor_model(newstates)\n",
        "    q1_t = target_critic([newstates, new_policy_actions])\n",
        "    q2_t = target_critic2([newstates, new_policy_actions])                    \n",
        "    tcritic_v = tf.reduce_min([q1_t,q2_t],axis=0) \n",
        "    newvalue = tcritic_v-entropy_scale*log_probs\n",
        "    q_hat = tf.stop_gradient(rewards + gamma*newvalue*(1-dones))\n",
        "    with tf.GradientTape(persistent=True) as tape1:\n",
        "        q1 = critic_model([states, actions])\n",
        "        q2 = critic2_model([states, actions]) \n",
        "        loss_c1 = tf.reduce_mean((q1 - q_hat)**2)\n",
        "        loss_c2 = tf.reduce_mean((q2 - q_hat)**2)\n",
        "    critic1_gradient = tape1.gradient(loss_c1, critic_model.trainable_variables)\n",
        "    critic2_gradient = tape1.gradient(loss_c2, critic2_model.trainable_variables)\n",
        "    critic_model.optimizer.apply_gradients(zip(critic1_gradient, critic_model.trainable_variables))\n",
        "    critic2_model.optimizer.apply_gradients(zip(critic2_gradient, critic2_model.trainable_variables))\n",
        "\n",
        "@tf.function    \n",
        "def update_actor(states):\n",
        "    entropy_scale = tf.convert_to_tensor(alpha)\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "        tape.watch(actor_model.trainable_variables)\n",
        "        _, new_policy_actions, log_probs = actor_model(states)\n",
        "        q1_n = critic_model([states, new_policy_actions])\n",
        "        q2_n = critic2_model([states, new_policy_actions])                    \n",
        "        critic_v = tf.reduce_min([q1_n,q2_n],axis=0)      \n",
        "        actor_loss = critic_v - entropy_scale*log_probs \n",
        "        actor_loss = -tf.reduce_mean(actor_loss)\n",
        "    actor_gradient = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "    actor_model.optimizer.apply_gradients(zip(actor_gradient, actor_model.trainable_variables))\n",
        "\n",
        "@tf.function\n",
        "def update_entropy(states):\n",
        "    _, _, log_probs= actor_model(states)\n",
        "    with tf.GradientTape() as tape:\n",
        "        alpha_loss = tf.reduce_mean(- alpha*tf.stop_gradient(log_probs + target_entropy))\n",
        "    alpha_grad = tape.gradient(alpha_loss, [log_alpha])\n",
        "    alpha_optimizer.apply_gradients(zip(alpha_grad, [log_alpha]))\n",
        "\n",
        "\n",
        "def train(total_iterations=total_iterations):\n",
        "    i = 0\n",
        "    mean_speed = 0\n",
        "    ep = 0\n",
        "    avg_reward = 0\n",
        "    while i<total_iterations:\n",
        "\n",
        "        prev_state = racer.reset()\n",
        "        episodic_reward = 0\n",
        "        mean_speed += prev_state[4]\n",
        "        done = False\n",
        "\n",
        "        while not(done):\n",
        "            i = i+1\n",
        "\n",
        "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "            _, action, _= actor_model(tf_prev_state)\n",
        "            state, reward, done = step(action)\n",
        "            \n",
        "            #we distinguish between termination with failure (state = None) and succesfull termination on track completion\n",
        "            #succesfull termination is stored as a normal tuple\n",
        "            fail = done and len(state)<5 \n",
        "            buffer.record((prev_state, action, reward, fail, state))\n",
        "            if not(done):\n",
        "                mean_speed += state[4]\n",
        "        \n",
        "            episodic_reward += reward\n",
        "\n",
        "            if buffer.buffer_counter>batch_size:\n",
        "                states,actions,rewards,dones,newstates = buffer.sample_batch()\n",
        "                states = tf.stack(tf.convert_to_tensor(states, dtype=tf.float32))\n",
        "                actions = tf.stack(tf.convert_to_tensor(actions, dtype=tf.float32))\n",
        "                rewards = tf.stack(tf.convert_to_tensor(rewards, dtype=tf.float32))\n",
        "                dones = tf.stack(tf.convert_to_tensor(dones, dtype=tf.float32))\n",
        "                newstates = tf.stack(tf.convert_to_tensor(newstates, dtype=tf.float32))\n",
        "                \n",
        "                update_critics(states, actions, rewards, dones, newstates)\n",
        "                update_actor(states)\n",
        "                update_entropy(states)\n",
        "                update_target(target_critic.variables, critic_model.variables, tau)\n",
        "                update_target(target_critic2.variables, critic2_model.variables, tau)\n",
        "                \n",
        "            prev_state = state\n",
        "            \n",
        "            if i%100 == 0:\n",
        "                avg_reward_list.append(avg_reward)\n",
        "\n",
        "        ep_reward_list.append(episodic_reward)\n",
        "\n",
        "        # Mean of last 40 episodes\n",
        "        avg_reward = np.mean(ep_reward_list[-40:])\n",
        "        print(\"Episode {}: Iterations {}, Avg. Reward = {}, Last reward = {}. Avg. speed = {}\".format(ep, i, avg_reward,episodic_reward,mean_speed/i))\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        if ep>0 and ep%40 == 0:\n",
        "            print(\"## Evaluating policy ##\")\n",
        "            tracks.metrics_run(actor_model, 10)\n",
        "        ep += 1\n",
        "\n",
        "    if total_iterations > 0:\n",
        "        if save_weights:\n",
        "            critic_model.save(weights_file_critic)\n",
        "            critic2_model.save(weights_file_critic2)\n",
        "            actor_model.save(weights_file_actor) \n",
        "        # Plotting Episodes versus Avg. Rewards\n",
        "        plt.plot(avg_reward_list)\n",
        "        plt.xlabel(\"Training steps x100\")\n",
        "        plt.ylabel(\"Avg. Episodic Reward\")\n",
        "        plt.ylim(-3.5,7)\n",
        "        plt.show(block=False)\n",
        "        plt.pause(0.001)\n",
        "        print(\"### SAC Training ended ###\")\n",
        "        print(\"Trained over {} steps\".format(i))\n",
        "\n",
        "if is_training:\n",
        "    start_t = datetime.now()\n",
        "    train()\n",
        "    end_t = datetime.now()\n",
        "    print(\"Time elapsed: {}\".format(end_t-start_t))"
      ],
      "metadata": {
        "id": "uRbC0eWACKF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To compete, we will be use the actor model\n",
        "sac_model = actor_model"
      ],
      "metadata": {
        "id": "c-XrO9WqDaro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compete!"
      ],
      "metadata": {
        "id": "gBI_9GYE_N-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Remember:\n",
        "\n",
        "Add save_count=1000 in Animate\n",
        "\n",
        "# Save as GIF\n",
        "anim.save('animation_car.gif', writer='pillow', fps=60)\n",
        "\n",
        "Since there is no Display on Colab\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GeytUMbiVS96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracks.newrun([ddpg_model, ddpg2_model, ppo_model, sac_model])"
      ],
      "metadata": {
        "id": "WJGCmhu-_SWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}